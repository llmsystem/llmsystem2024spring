| Dates | Topic                                                                                   | Reading/Content                                                                                                                                                                                                                                                              | Homework                                                   |
| ----- | --------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------- |
| 1/17  | Introduction to LLM [[slides]](/slides/llmsys-01-intro.pdf)                             |                                                                                                                                                                                                                                                                              | [HW1 out](/assignments/11868_LLM_Systems_Assignment_1.pdf) |
| 1/22  | GPU Programming Basics [[slides]](/slides/llmsys-02-hw-comp.pdf)                        | Chap 2,3 of [Programming Massively Parallel Processors, 3rd Ed](https://cmu.primo.exlibrisgroup.com/permalink/01CMU_INST/6lpsnm/alma991019904889504436)                                                                                                                      |                                                            |
|       | Learning algorithm and Auto Differentiation  [[slides]](/slides/llmsys-03-autodiff.pdf) | [Auto Diff survery](https://arxiv.org/abs/1502.05767)                                                                                                                                                                                                                        |                                                            |
| 1/29  | Deep Learning Frameworks Design Principles                                              | [Tensorflow](https://www.usenix.org/system/files/conference/osdi16/osdi16-abadi.pdf)                                                                                                                                                                                         |                                                            |
|       | Transformer                                                                             | Attention is all you need, Annotated Transformer                                                                                                                                                                                                                             |                                                            |
| 2/5   | Fast Tokenization and Embeddings                                                        | BPE, Sentence-Piece, Word-piece, Token and Positional Embeddings.                                                                                                                                                                                                            | HW1 due                                                    |
|       | Decoding - sampling & beam search                                                       | Top-k sampling, Beam search, Diverse bean search, Constrained Generation                                                                                                                                                                                                     |                                                            |
| 2/12  | GPU Architecture for DL                                                                 | basic introduction of A100/H100 GPU                                                                                                                                                                                                                                          | Guest(?)                                                   |
|       | GPU Acceleration 1                                                                      | LightSeq                                                                                                                                                                                                                                                                     |                                                            |
| 2/19  | Efficient Sequence Decoding on GPU                                                      | LightSeq, FlexGen                                                                                                                                                                                                                                                            | HW2 due                                                    |
|       | Accelerating Backward Computation on GPU                                                |                                                                                                                                                                                                                                                                              |                                                            |
| 2/26  | Distributed Model Training                                                              | PyTorch FSDP, Pytorch notebook implementations, Stabilizing LLM training                                                                                                                                                                                                     |                                                            |
|       | Communication Efficient Distributed Training, popular frameworks                        | DeepSpeed, gpt-neox, megatron                                                                                                                                                                                                                                                |                                                            |
| 3/4   | spring break                                                                            |                                                                                                                                                                                                                                                                              |                                                            |
| 3/11  | Model Quantization and Compression                                                      | GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers                                                                                                                                                                                            | HW3 Due                                                    |
|       | Efficient fine-tuning for Large Models                                                  | LORA, QLoRA, gradient checkpoint                                                                                                                                                                                                                                             |                                                            |
| 3/18  | Model Serving Service and GPU Inference Frameworks                                      | LightLLM and Triton                                                                                                                                                                                                                                                          |                                                            |
|       | CPU based Serving, LLM Operation and Monitor                                            | gglm, llama.cpp, MLC, https://github.com/tensorchord/Awesome-LLMOps                                                                                                                                                                                                          |                                                            |
| 3/25  | Advanced Large Model Serving and GPU Inference frameworks                               | Orca: A Distributed Serving System for Transformer-Based Generative Models                                                                                                                                                                                                   | HW4 Due                                                    |
|       | PageAttention                                                                           | vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention, AlpaServe                                                                                                                                                                                                       |                                                            |
| 4/1   | GPU just-in-time compilation                                                            | Compiling machine learning programs via high-level tracing (JAX)                                                                                                                                                                                                             |                                                            |
|       | Large models, Sparse Routing, Mixture-of-Expert, and Multiway Networks                  | Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer, GShard, DeepSpeed-MOE, Tutul                                                                                                                                                                |                                                            |
| 4/8   | Efficient Attention and Memory Optimization for LLMs                                    | FlashAttention, Multi-query Attention, ZeRO                                                                                                                                                                                                                                  | HW5 Due                                                    |
|       | Longe Context and Extremely Long Sequence Generation                                    | Rethinking Attention with Performers, Blockwise Parallel Transformer for Large Context Models, Scaling Transformer to 1M tokens and beyond with RMT                                                                                                                          |                                                            |
| 4/15  | Efficient Vector Database and Retrieval-augmented Language Models                       | REALM: Retrieval-Augmented Language Model Pre-Training, Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks                                                                                                                                                     |                                                            |
|       | Nearest Vector Search for Embeddings                                                    | Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs, Filtered − DiskANN: Graph Algorithms for Approximate Nearest Neighbor Search with Filters, Accelerating Large-Scale Inference with Anisotropic Vector Quantization |                                                            |
| 4/22  | Speculative Decoding                                                                    | Fast Inference from Transformers via Speculative Decoding; Accelerating Large Language Model Decoding with Speculative Sampling                                                                                                                                              |
|       | Multimodal LLMs (visual language and speech language)                                   | Flamingo: a Visual Language Model for Few-Shot Learning, SeamlessM4T—Massively Multilingual & Multimodal Machine Translation                                                                                                                                                 |                                                            |
| 4/28  | Final project presentation                                                              |                                                                                                                                                                                                                                                                              |                                                            |
